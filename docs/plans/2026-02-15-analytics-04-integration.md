# Integration & End-to-End Testing

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers-executing-plans to implement this plan task-by-task.

**Goal:** å®ŒæˆåŽç«¯ã€SDKã€å‰ç«¯ä¸‰è€…çš„é›†æˆè”è°ƒï¼ŒéªŒè¯ SDKâ†’Ingestâ†’DBâ†’Dashboard å…¨é“¾è·¯æ•°æ®æµè½¬ï¼Œå¹¶é€šè¿‡ E2E æµ‹è¯•ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§ã€‚

**Architecture:** æ¨¡æ‹ŸçœŸå®ž Skill ä½¿ç”¨åœºæ™¯ï¼Œé€šè¿‡ SDK å‘é€äº‹ä»¶åˆ° HTTP Ingest Serverï¼ŒéªŒè¯äº‹ä»¶æ­£ç¡®å†™å…¥ SQLiteï¼Œå†é€šè¿‡å‰ç«¯ Dashboard å±•ç¤ºæ•°æ®ã€‚åŒ…å«å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€E2E æµ‹è¯•ã€‚

**Tech Stack:** Rust (rusqlite, tiny_http), TypeScript SDK, React 19, Tauri invoke, curl (æµ‹è¯•å·¥å…·)

**å‰ç½®ä¾èµ–:** å¿…é¡»å…ˆå®Œæˆ 01-backendã€02-sdkã€03-fronten ä¸‰ä¸ªæ¨¡å—çš„å®žæ–½ã€‚

**å¹¶è¡Œè¯´æ˜Ž:** æœ¬æ¨¡å—ä¾èµ–å‰ä¸‰ä¸ªæ¨¡å—å…¨éƒ¨å®Œæˆï¼Œä¸²è¡Œæ‰§è¡Œã€‚åŒ…å« 6 ä¸ª Taskï¼Œæ¯ä¸ª Task å¯ç‹¬ç«‹æäº¤ã€‚

---

## Task 1: éªŒè¯ HTTP Ingest Server å¯ç”¨æ€§

**Files:**
- Test: `src-tauri/src/core/analytics_ingest.rs`

**Step 1: å¯åŠ¨ Tauri åº”ç”¨**

Run: `cargo tauri dev`
Expected: åº”ç”¨æˆåŠŸå¯åŠ¨ï¼ŒHTTP Ingest Server ç›‘å¬ `127.0.0.1:19823`

**Step 2: æµ‹è¯• Ingest ç«¯ç‚¹**

Run: `curl -X POST http://127.0.0.1:19823/api/v1/ingest -H "Content-Type: application/json" -d '{"skill_id":"test-skill","event_type":"call","timestamp":1739596800,"status":"success","latency_ms":100}'`
Expected: è¿”å›ž `{"status":"accepted"}` æˆ– `200 OK`

**Step 3: éªŒè¯äº‹ä»¶å†™å…¥æ•°æ®åº“**

Run: `sqlite3 $HOME/Library/Application\ Support/com.example.skills-hub/skills_hub.db "SELECT * FROM skill_events ORDER BY event_id DESC LIMIT 1;"`
Expected: æŸ¥è¯¢åˆ°åˆšæ’å…¥çš„äº‹ä»¶è®°å½•

**ðŸ” æ£€æµ‹ç‚¹:** curl è¯·æ±‚è¿”å›žæˆåŠŸï¼Œæ•°æ®åº“ä¸­æœ‰å¯¹åº”è®°å½•
**âœ… éªŒæ”¶æ ‡å‡†:**
- HTTP ç«¯ç‚¹å“åº” 200
- è¿”å›ž JSON åŒ…å« `status: "accepted"`
- æ•°æ®åº“ä¸­ `skill_id = "test-skill"` çš„äº‹ä»¶å­˜åœ¨

---

## Task 2: SDK é›†æˆæµ‹è¯•

**Files:**
- Create: `packages/analytics/tests/integration.test.ts`

**Step 1: åˆ›å»ºé›†æˆæµ‹è¯•**

```typescript
import { describe, it, expect, beforeEach, afterEach } from 'vitest'
import { Tracker } from '../src/tracker'
import { Transport } from '../src/transport'

describe('SDK Integration Tests', () => {
  const TEST_ENDPOINT = 'http://127.0.0.1:19823/api/v1/ingest'
  const TEST_SKILL_ID = 'integration-test-skill'

  let tracker: Tracker

  beforeEach(() => {
    tracker = new Tracker({
      skillId: TEST_SKILL_ID,
      endpoint: TEST_ENDPOINT,
      bufferSize: 5,
      flushInterval: 1000,
    })
  })

  afterEach(async () => {
    await tracker.shutdown()
  })

  it('should send event to ingest server', async () => {
    await tracker.trackCall({
      status: 'success',
      latencyMs: 150,
      metadata: { test: 'integration' },
    })

    await new Promise((resolve) => setTimeout(resolve, 2000))

    const response = await fetch(`${TEST_ENDPOINT}?skill_id=${TEST_SKILL_ID}`)
    expect(response.ok).toBeTruthy()
  })

  it('should buffer and flush events', async () => {
    for (let i = 0; i < 10; i++) {
      await tracker.trackCall({
        status: i % 2 === 0 ? 'success' : 'failure',
        latencyMs: 100 + i * 10,
      })
    }

    await new Promise((resolve) => setTimeout(resolve, 2000))

    const response = await fetch('http://127.0.0.1:19823/api/v1/health')
    expect(response.ok).toBeTruthy()
  })

  it('should handle network errors gracefully', async () => {
    const errorTracker = new Tracker({
      skillId: 'error-test',
      endpoint: 'http://127.0.0.1:99999/invalid',
      bufferSize: 2,
    })

    await errorTracker.trackCall({ status: 'success', latencyMs: 100 })
    await new Promise((resolve) => setTimeout(resolve, 500))

    await errorTracker.shutdown()
  })
})
```

**Step 2: è¿è¡Œé›†æˆæµ‹è¯•**

Run: `cd packages/analytics && npm test`
Expected: æ‰€æœ‰æµ‹è¯•é€šè¿‡

**Step 3: Commit**

```bash
git add packages/analytics/tests/integration.test.ts
git commit -m "test(analytics-sdk): add integration tests for ingest server"
```

**ðŸ” æ£€æµ‹ç‚¹:** æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡
**âœ… éªŒæ”¶æ ‡å‡†:**
- äº‹ä»¶æˆåŠŸå‘é€åˆ° Ingest Server
- ç¼“å†²æœºåˆ¶æ­£å¸¸å·¥ä½œ
- ç½‘ç»œé”™è¯¯è¢«æ­£ç¡®å¤„ç†

---

## Task 3: åŽç«¯é›†æˆæµ‹è¯•

**Files:**
- Create: `src-tauri/tests/analytics_integration_test.rs`

**Step 1: åˆ›å»ºé›†æˆæµ‹è¯•**

```rust
#[cfg(test)]
mod analytics_integration_tests {
    use super::*;
    use chrono::Utc;
    use rusqlite::Connection;

    fn get_test_db_path() -> String {
        format!("/tmp/skills_hub_test_{}.db", std::process::id())
    }

    fn setup_test_db() -> Connection {
        let db_path = get_test_db_path();
        let _ = std::fs::remove_file(&db_path);
        let conn = Connection::open(&db_path).unwrap();
        create_analytics_tables(&conn).unwrap();
        conn
    }

    #[test]
    fn test_event_crud() {
        let conn = setup_test_db();

        let event = SkillEvent {
            event_id: 1,
            skill_id: "test-skill".to_string(),
            event_type: "call".to_string(),
            timestamp: Utc::now().timestamp(),
            status: "success".to_string(),
            latency_ms: 100,
            error_message: None,
            user_id: None,
            session_id: None,
            metadata: None,
            created_at: Utc::now().timestamp(),
        };

        insert_event(&conn, &event).unwrap();

        let retrieved = get_event_by_id(&conn, 1).unwrap();
        assert_eq!(retrieved.skill_id, "test-skill");
        assert_eq!(retrieved.status, "success");

        std::fs::remove_file(get_test_db_path()).unwrap();
    }

    #[test]
    fn test_aggregation_queries() {
        let conn = setup_test_db();

        for i in 0..10 {
            let event = SkillEvent {
                event_id: (i + 1) as i64,
                skill_id: "test-skill".to_string(),
                event_type: "call".to_string(),
                timestamp: Utc::now().timestamp() - (i * 86400),
                status: if i % 3 == 0 { "failure" } else { "success" }.to_string(),
                latency_ms: 100 + i * 10,
                error_message: None,
                user_id: Some(format!("user-{}", i % 3)),
                session_id: None,
                metadata: None,
                created_at: Utc::now().timestamp(),
            };
            insert_event(&conn, &event).unwrap();
        }

        let overview = get_overview_stats(&conn).unwrap();
        assert_eq!(overview.total_events, 10);
        assert_eq!(overview.successful_events, 7);
        assert_eq!(overview.failed_events, 3);

        std::fs::remove_file(get_test_db_path()).unwrap();
    }

    #[test]
    fn test_alert_detection() {
        let conn = setup_test_db();

        let mut events = vec![];
        for i in 0..20 {
            events.push(SkillEvent {
                event_id: (i + 1) as i64,
                skill_id: "alert-test".to_string(),
                event_type: "call".to_string(),
                timestamp: Utc::now().timestamp() - (i * 60),
                status: if i < 15 { "success" } else { "failure" }.to_string(),
                latency_ms: 100,
                error_message: None,
                user_id: None,
                session_id: None,
                metadata: None,
                created_at: Utc::now().timestamp(),
            });
        }

        for event in &events {
            insert_event(&conn, event).unwrap();
        }

        let alerts = detect_alerts(&conn).unwrap();
        assert!(!alerts.is_empty());

        std::fs::remove_file(get_test_db_path()).unwrap();
    }
}
```

**Step 2: è¿è¡Œé›†æˆæµ‹è¯•**

Run: `cargo test analytics_integration_tests`
Expected: æ‰€æœ‰æµ‹è¯•é€šè¿‡

**Step 3: Commit**

```bash
git add src-tauri/tests/analytics_integration_test.rs
git commit -m "test(analytics-backend): add integration tests for CRUD and aggregation"
```

**ðŸ” æ£€æµ‹ç‚¹:** æ‰€æœ‰åŽç«¯é›†æˆæµ‹è¯•é€šè¿‡
**âœ… éªŒæ”¶æ ‡å‡†:**
- äº‹ä»¶ CRUD æ“ä½œæ­£å¸¸
- èšåˆæŸ¥è¯¢è¿”å›žæ­£ç¡®ç»“æžœ
- å‘Šè­¦æ£€æµ‹é€»è¾‘è§¦å‘

---

## Task 4: å‰ç«¯ E2E æµ‹è¯•

**Files:**
- Create: `tests/analytics_e2e.test.ts`

**Step 1: åˆ›å»º E2E æµ‹è¯•**

```typescript
import { test, expect } from '@playwright/test'

test.describe('Analytics Dashboard E2E', () => {
  test.beforeEach(async ({ page }) => {
    await page.goto('http://localhost:1420')
  })

  test('should display analytics dashboard', async ({ page }) => {
    await page.click('text=ðŸ“Š Analytics')
    await expect(page.locator('h1:has-text("Analytics Dashboard")')).toBeVisible()
  })

  test('should show overview cards', async ({ page }) => {
    await page.click('text=ðŸ“Š Analytics')
    await expect(page.locator('text=Total Calls')).toBeVisible()
    await expect(page.locator('text=Success Rate')).toBeVisible()
    await expect(page.locator('text=Avg Latency')).toBeVisible()
    await expect(page.locator('text=Active Users')).toBeVisible()
  })

  test('should display charts', async ({ page }) => {
    await page.click('text=ðŸ“Š Analytics')
    await expect(page.locator('text=Daily Trend')).toBeVisible()
    await expect(page.locator('text=Success Rate')).toBeVisible()
  })

  test('should handle refresh button', async ({ page }) => {
    await page.click('text=ðŸ“Š Analytics')
    await page.click('button:has-text("Refresh")')
    await expect(page.locator('button:has-text("Loading...")')).toBeVisible()
    await expect(page.locator('button:has-text("Refresh")')).toBeVisible({ timeout: 5000 })
  })

  test('should acknowledge alerts', async ({ page }) => {
    await page.click('text=ðŸ“Š Analytics')
    const alertDismissButton = page.locator('button:has-text("Dismiss")').first()
    if (await alertDismissButton.isVisible()) {
      await alertDismissButton.click()
      await expect(alertDismissButton).not.toBeVisible()
    }
  })
})
```

**Step 2: è¿è¡Œ E2E æµ‹è¯•**

Run: `npm run test:e2e`
Expected: æ‰€æœ‰ E2E æµ‹è¯•é€šè¿‡

**Step 3: Commit**

```bash
git add tests/analytics_e2e.test.ts
git commit -m "test(analytics-frontend): add E2E tests for dashboard"
```

**ðŸ” æ£€æµ‹ç‚¹:** æ‰€æœ‰ E2E æµ‹è¯•é€šè¿‡
**âœ… éªŒæ”¶æ ‡å‡†:**
- Dashboard é¡µé¢æ­£å¸¸åŠ è½½
- æ‰€æœ‰ç»„ä»¶å¯è§
- Refresh æŒ‰é’®åŠŸèƒ½æ­£å¸¸
- å‘Šè­¦ç¡®è®¤åŠŸèƒ½æ­£å¸¸

---

## Task 5: å…¨é“¾è·¯æ•°æ®æµæµ‹è¯•

**Files:**
- Create: `scripts/test_full_pipeline.sh`

**Step 1: åˆ›å»ºå…¨é“¾è·¯æµ‹è¯•è„šæœ¬**

```bash
#!/bin/bash

set -e

echo "ðŸš€ Starting Full Pipeline Test..."

# 1. æ¸…ç†æµ‹è¯•æ•°æ®
echo "ðŸ“ Cleaning test data..."
DB_PATH="$HOME/Library/Application Support/com.example.skills-hub/skills_hub.db"
sqlite3 "$DB_PATH" "DELETE FROM skill_events WHERE skill_id = 'pipeline-test';"

# 2. å¯åŠ¨ Tauri åº”ç”¨ï¼ˆå‡è®¾å·²åœ¨åŽå°è¿è¡Œï¼‰
# cargo tauri dev &

# 3. ä½¿ç”¨ SDK å‘é€äº‹ä»¶
echo "ðŸ“¦ Sending events via SDK..."
curl -X POST http://127.0.0.1:19823/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "skill_id": "pipeline-test",
    "event_type": "call",
    "timestamp": 1739596800,
    "status": "success",
    "latency_ms": 120,
    "user_id": "user-1",
    "session_id": "session-1",
    "metadata": {"test": "pipeline"}
  }'

# 4. ç­‰å¾…äº‹ä»¶å†™å…¥
echo "â³ Waiting for event persistence..."
sleep 2

# 5. éªŒè¯æ•°æ®åº“
echo "ðŸ” Verifying database..."
COUNT=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM skill_events WHERE skill_id = 'pipeline-test';")
if [ "$COUNT" -eq "1" ]; then
  echo "âœ… Event persisted successfully!"
else
  echo "âŒ Event persistence failed! Count: $COUNT"
  exit 1
fi

# 6. æµ‹è¯• Tauri Command
echo "ðŸŽ¯ Testing Tauri commands..."
# è¿™é‡Œéœ€è¦é€šè¿‡ Tauri çš„ IPC è°ƒç”¨ï¼Œå®žé™…æµ‹è¯•ä¸­å¯ä»¥ä½¿ç”¨ Tauri çš„æµ‹è¯•å·¥å…·
# ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æŽ¥æŸ¥è¯¢æ•°æ®åº“éªŒè¯èšåˆé€»è¾‘
TOTAL=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM skill_events;")
SUCCESS=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM skill_events WHERE status = 'success';")
echo "Total events: $TOTAL, Success: $SUCCESS"

# 7. æ¸…ç†
echo "ðŸ§¹ Cleaning up..."
sqlite3 "$DB_PATH" "DELETE FROM skill_events WHERE skill_id = 'pipeline-test';"

echo "âœ¨ Full pipeline test completed successfully!"
```

**Step 2: è¿è¡Œå…¨é“¾è·¯æµ‹è¯•**

Run: `chmod +x scripts/test_full_pipeline.sh && ./scripts/test_full_pipeline.sh`
Expected: è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼Œæ‰€æœ‰æ­¥éª¤é€šè¿‡

**Step 3: Commit**

```bash
git add scripts/test_full_pipeline.sh
git commit -m "test(analytics): add full pipeline integration test script"
```

**ðŸ” æ£€æµ‹ç‚¹:** è„šæœ¬æ‰§è¡Œæ— é”™è¯¯
**âœ… éªŒæ”¶æ ‡å‡†:**
- äº‹ä»¶æˆåŠŸé€šè¿‡ HTTP Ingest æŽ¥æ”¶
- äº‹ä»¶æ­£ç¡®å†™å…¥ SQLite
- æ•°æ®åº“æŸ¥è¯¢è¿”å›žé¢„æœŸç»“æžœ
- æ¸…ç†æ­¥éª¤æˆåŠŸæ‰§è¡Œ

---

## Task 6: æ€§èƒ½å’ŒåŽ‹åŠ›æµ‹è¯•

**Files:**
- Create: `scripts/test_performance.sh`

**Step 1: åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬**

```bash
#!/bin/bash

set -e

echo "âš¡ Starting Performance Test..."

CONCURRENT_REQUESTS=100
TOTAL_REQUESTS=1000
ENDPOINT="http://127.0.0.1:19823/api/v1/ingest"

echo "ðŸ“Š Testing $TOTAL_REQUESTS requests with $CONCURRENT_REQUESTS concurrent connections..."

# ä½¿ç”¨ Apache Bench è¿›è¡ŒåŽ‹åŠ›æµ‹è¯•
ab -n $TOTAL_REQUESTS -c $CONCURRENT_REQUESTS -p /tmp/ingest_payload.json -T application/json $ENDPOINT

echo ""
echo "ðŸ“ˆ Performance test completed!"
echo "Check the output above for:"
echo "  - Requests per second"
echo "  - Time per request"
echo "  - Failed requests (should be 0)"
```

**Step 2: åˆ›å»ºæµ‹è¯•è´Ÿè½½**

Run: `cat > /tmp/ingest_payload.json << 'EOF'
{
  "skill_id": "perf-test",
  "event_type": "call",
  "timestamp": 1739596800,
  "status": "success",
  "latency_ms": 100
}
EOF`

**Step 3: è¿è¡Œæ€§èƒ½æµ‹è¯•**

Run: `chmod +x scripts/test_performance.sh && ./scripts/test_performance.sh`
Expected: è¯·æ±‚æˆåŠŸçŽ‡ 100%ï¼Œå“åº”æ—¶é—´åœ¨å¯æŽ¥å—èŒƒå›´å†…

**Step 4: Commit**

```bash
git add scripts/test_performance.sh
git commit -m "test(analytics): add performance and load testing script"
```

**ðŸ” æ£€æµ‹ç‚¹:** ab å·¥å…·è¾“å‡ºæ˜¾ç¤º 0 Failed requests
**âœ… éªŒæ”¶æ ‡å‡†:**
- 1000 ä¸ªè¯·æ±‚å…¨éƒ¨æˆåŠŸ
- å¹³å‡å“åº”æ—¶é—´ &lt; 100ms
- æ—  5xx é”™è¯¯

---

## ðŸŽ¯ æœ€ç»ˆéªŒæ”¶æ¸…å•

### åŠŸèƒ½éªŒæ”¶
- [ ] HTTP Ingest Server æ­£å¸¸æŽ¥æ”¶äº‹ä»¶
- [ ] äº‹ä»¶æ­£ç¡®å†™å…¥ SQLite æ•°æ®åº“
- [ ] SDK é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] åŽç«¯é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] å‰ç«¯ E2E æµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] å…¨é“¾è·¯æ•°æ®æµæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•æ»¡è¶³è¦æ±‚

### æ•°æ®ä¸€è‡´æ€§éªŒæ”¶
- [ ] SDK å‘é€çš„äº‹ä»¶ä¸Žæ•°æ®åº“ä¸­çš„è®°å½•ä¸€è‡´
- [ ] èšåˆæŸ¥è¯¢ç»“æžœå‡†ç¡®
- [ ] å‘Šè­¦æ£€æµ‹é€»è¾‘æ­£ç¡®è§¦å‘
- [ ] å‰ç«¯ Dashboard æ˜¾ç¤ºçš„æ•°æ®ä¸Žæ•°æ®åº“ä¸€è‡´

### æ€§èƒ½éªŒæ”¶
- [ ] Ingest Server å“åº”æ—¶é—´ &lt; 50ms (å•è¯·æ±‚)
- [ ] æ”¯æŒå¹¶å‘ 100+ è¯·æ±‚
- [ ] æ•°æ®åº“æŸ¥è¯¢ &lt; 100ms
- [ ] å‰ç«¯é¡µé¢åŠ è½½ &lt; 2s

### ç¨³å®šæ€§éªŒæ”¶
- [ ] ç½‘ç»œé”™è¯¯ä¸å½±å“åº”ç”¨ç¨³å®šæ€§
- [ ] æ•°æ®åº“è¿žæŽ¥æ± æ­£å¸¸å·¥ä½œ
- [ ] å‰ç«¯é”™è¯¯è¾¹ç•Œæ­£å¸¸æ•èŽ·å¼‚å¸¸
- [ ] æ— å†…å­˜æ³„æ¼

### æ–‡æ¡£éªŒæ”¶
- [ ] SDK ä½¿ç”¨æ–‡æ¡£å®Œæ•´
- [ ] API æ–‡æ¡£æ›´æ–°
- [ ] æµ‹è¯•è¦†ç›–çŽ‡æŠ¥å‘Šç”Ÿæˆ

---

## ðŸ“‹ åŽç»­ä¼˜åŒ–å»ºè®®

### çŸ­æœŸä¼˜åŒ– (1-2 å‘¨)
- [ ] æ·»åŠ æ›´å¤šå›¾è¡¨ç±»åž‹ï¼ˆçƒ­åŠ›å›¾ã€æ¼æ–—å›¾ï¼‰
- [ ] å®žçŽ°æ•°æ®å¯¼å‡ºåŠŸèƒ½ (CSV/JSON)
- [ ] æ·»åŠ è‡ªå®šä¹‰æ—¶é—´èŒƒå›´é€‰æ‹©å™¨
- [ ] å®žçŽ°å®žæ—¶æ•°æ®åˆ·æ–° (WebSocket)

### ä¸­æœŸä¼˜åŒ– (1-2 æœˆ)
- [ ] æ·»åŠ  A/B æµ‹è¯•æ”¯æŒ
- [ ] å®žçŽ°æˆæœ¬è¿½è¸ªå’Œé¢„ç®—å‘Šè­¦
- [ ] æ·»åŠ ç”¨æˆ·è¡Œä¸ºåˆ†æž
- [ ] å®žçŽ° Skill ä¾èµ–åˆ†æžå¯è§†åŒ–

### é•¿æœŸä¼˜åŒ– (3-6 æœˆ)
- [ ] æ•°æ®å½’æ¡£å’Œæ¸…ç†ç­–ç•¥
- [ ] åˆ†å¸ƒå¼å­˜å‚¨æ”¯æŒ (PostgreSQL/TimescaleDB)
- [ ] æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹
- [ ] è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

---

**Plan complete and saved to `docs/plans/2026-02-15-analytics-04-integration.md`.**

**All 4 implementation plans are now complete!**

**Execution Options:**

**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration

**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints

**Which approach?**

**If Subagent-Driven chosen:**
- **REQUIRED SUB-SKILL:** Use superpowers-subagent-driven-development
- Stay in this session
- Fresh subagent per task + code review

**If Parallel Session chosen:**
- Guide them to open new session in worktree
- **REQUIRED SUB-SKILL:** New session uses superpowers-executing-plans
